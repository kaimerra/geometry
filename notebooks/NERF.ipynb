{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab264ac-043c-4959-a73b-187285134909",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1fb22-e89d-473f-9865-a3134195bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaigeo import datasets\n",
    "import numpy as np\n",
    "from kaigeo.nerf_models import NeuralRadianceField, huber\n",
    "import torch\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Volumes\n",
    "from pytorch3d.transforms import so3_exp_map\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, \n",
    "    NDCGridRaysampler,\n",
    "    MonteCarloRaysampler,\n",
    "    EmissionAbsorptionRaymarcher,\n",
    "    ImplicitRenderer,\n",
    "    RayBundle,\n",
    "    ray_bundle_to_ray_points,\n",
    "    BlendParams,\n",
    "    FoVPerspectiveCameras,\n",
    "    MeshRasterizer,\n",
    "    MeshRenderer,\n",
    "    PointLights,\n",
    "    RasterizationSettings,\n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    look_at_view_transform,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from plot_image_grid import image_grid\n",
    "    \n",
    "except ModuleNotFoundError:\n",
    "    !wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\n",
    "    from plot_image_grid import image_grid\n",
    "\n",
    "\n",
    "# obtain the utilized device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    print(\n",
    "        'Please note that NeRF is a resource-demanding method.'\n",
    "        + ' Running this notebook on CPU will be extremely slow.'\n",
    "        + ' We recommend running the example on a GPU'\n",
    "        + ' with at least 10 GB of memory.'\n",
    "    )\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeca366-9118-4166-9e72-3775c22e4608",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd324cdd-350b-433d-babe-64704471f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datasets.load_session2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd6211-59ad-4a1c-8841-969c3236f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye = s.eye\n",
    "forward = s.forward\n",
    "look = s.look\n",
    "player = s.player\n",
    "target_images = s.target_images\n",
    "\n",
    "R, T = look_at_view_transform(eye = eye, at = eye+forward) \n",
    "target_cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "target_silhouettes = target_images.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c272d-276e-4124-9d62-76c1edcd2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig = plt.figure(figsize = (10,5))\n",
    "frames = [] # for storing the generated images\n",
    "# fig, ax = plt.subplots(1,2)\n",
    "for i in range(len(target_images)):\n",
    "    frames.append([plt.imshow(target_images[i], cmap=cm.Greys_r,animated=True)])\n",
    "    \n",
    "\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, frames, interval=30, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "# ani.save('movie.mp4')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08ff4a-45fb-4b96-a0df-0aff8c872a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = [pos[1] for pos in eye]\n",
    "xdata = [pos[0] for pos in eye]\n",
    "ydata = [pos[2] for pos in eye]\n",
    "ax.scatter3D(xdata, ydata, zdata, c = np.linspace(0,1,len(target_images)), cmap = 'seismic');\n",
    "ax.quiver(xdata, ydata, zdata, forward[:,0], forward[:,2], forward[:,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4976e-3372-448c-8925-10c944130bfb",
   "metadata": {},
   "source": [
    "# Initialize renderers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23eab44-8f78-4419-b7c3-a4b9d09c0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_size describes the size of both sides of the \n",
    "# rendered images in pixels. Since an advantage of \n",
    "# Neural Radiance Fields are high quality renders\n",
    "# with a significant amount of details, we render\n",
    "# the implicit function at double the size of \n",
    "# target images.\n",
    "render_size = target_images.shape[1] * 2\n",
    "\n",
    "# Our rendered scene is centered around (0,0,0) \n",
    "# and is enclosed inside a bounding box\n",
    "# whose side is roughly equal to 3.0 (world units).\n",
    "volume_extent_world = 3.0\n",
    "\n",
    "# 1) Instantiate the raysamplers.\n",
    "\n",
    "# Here, NDCGridRaysampler generates a rectangular image\n",
    "# grid of rays whose coordinates follow the PyTorch3D\n",
    "# coordinate conventions.\n",
    "raysampler_grid = NDCGridRaysampler(\n",
    "    image_height=render_size,\n",
    "    image_width=render_size,\n",
    "    n_pts_per_ray=128,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "\n",
    "# MonteCarloRaysampler generates a random subset \n",
    "# of `n_rays_per_image` rays emitted from the image plane.\n",
    "raysampler_mc = MonteCarloRaysampler(\n",
    "    min_x = -1.0,\n",
    "    max_x = 1.0,\n",
    "    min_y = -1.0,\n",
    "    max_y = 1.0,\n",
    "    n_rays_per_image=750,\n",
    "    n_pts_per_ray=128,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "\n",
    "# 2) Instantiate the raymarcher.\n",
    "# Here, we use the standard EmissionAbsorptionRaymarcher \n",
    "# which marches along each ray in order to render\n",
    "# the ray into a single 3D color vector \n",
    "# and an opacity scalar.\n",
    "raymarcher = EmissionAbsorptionRaymarcher()\n",
    "\n",
    "# Finally, instantiate the implicit renders\n",
    "# for both raysamplers.\n",
    "renderer_grid = ImplicitRenderer(\n",
    "    raysampler=raysampler_grid, raymarcher=raymarcher,\n",
    ")\n",
    "renderer_mc = ImplicitRenderer(\n",
    "    raysampler=raysampler_mc, raymarcher=raymarcher,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1e1f7-9d90-48e1-9468-a719f3e052b8",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8ba29-7acf-44bc-a7f3-0906ca626947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_images_at_mc_locs(target_images, sampled_rays_xy):\n",
    "    \"\"\"\n",
    "    Given a set of Monte Carlo pixel locations `sampled_rays_xy`,\n",
    "    this method samples the tensor `target_images` at the\n",
    "    respective 2D locations.\n",
    "    \n",
    "    This function is used in order to extract the colors from\n",
    "    ground truth images that correspond to the colors\n",
    "    rendered using `MonteCarloRaysampler`.\n",
    "    \"\"\"\n",
    "    ba = target_images.shape[0]\n",
    "    dim = target_images.shape[-1]\n",
    "    spatial_size = sampled_rays_xy.shape[1:-1]\n",
    "    # In order to sample target_images, we utilize\n",
    "    # the grid_sample function which implements a\n",
    "    # bilinear image sampler.\n",
    "    # Note that we have to invert the sign of the \n",
    "    # sampled ray positions to convert the NDC xy locations\n",
    "    # of the MonteCarloRaysampler to the coordinate\n",
    "    # convention of grid_sample.\n",
    "    images_sampled = torch.nn.functional.grid_sample(\n",
    "        target_images.permute(0, 3, 1, 2), \n",
    "        -sampled_rays_xy.view(ba, -1, 1, 2),  # note the sign inversion\n",
    "        align_corners=True\n",
    "    )\n",
    "    return images_sampled.permute(0, 2, 3, 1).view(\n",
    "        ba, *spatial_size, dim\n",
    "    )\n",
    "\n",
    "def show_full_render(\n",
    "    neural_radiance_field, camera,\n",
    "    target_image, target_silhouette,\n",
    "    loss_history_color, loss_history_sil,\n",
    "):\n",
    "    \"\"\"\n",
    "    This is a helper function for visualizing the\n",
    "    intermediate results of the learning. \n",
    "    \n",
    "    Since the `NeuralRadianceField` suffers from\n",
    "    a large memory footprint, which does not let us\n",
    "    render the full image grid in a single forward pass,\n",
    "    we utilize the `NeuralRadianceField.batched_forward`\n",
    "    function in combination with disabling the gradient caching.\n",
    "    This chunks the set of emitted rays to batches and \n",
    "    evaluates the implicit function on one batch at a time\n",
    "    to prevent GPU memory overflow.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prevent gradient caching.\n",
    "    with torch.no_grad():\n",
    "        # Render using the grid renderer and the\n",
    "        # batched_forward function of neural_radiance_field.\n",
    "        rendered_image_silhouette, _ = renderer_grid(\n",
    "            cameras=camera, \n",
    "            volumetric_function=neural_radiance_field.batched_forward\n",
    "        )\n",
    "        # Split the rendering result to a silhouette render\n",
    "        # and the image render.\n",
    "        rendered_image, rendered_silhouette = (\n",
    "            rendered_image_silhouette[0].split([3, 1], dim=-1)\n",
    "        )\n",
    "        \n",
    "    # Generate plots.\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    ax = ax.ravel()\n",
    "    clamp_and_detach = lambda x: x.clamp(0.0, 1.0).cpu().detach().numpy()\n",
    "    ax[0].plot(list(range(len(loss_history_color))), loss_history_color, linewidth=1)\n",
    "    ax[1].imshow(clamp_and_detach(rendered_image))\n",
    "    ax[2].imshow(clamp_and_detach(rendered_silhouette[..., 0]))\n",
    "    ax[3].plot(list(range(len(loss_history_sil))), loss_history_sil, linewidth=1)\n",
    "    ax[4].imshow(clamp_and_detach(target_image))\n",
    "    ax[5].imshow(clamp_and_detach(target_silhouette))\n",
    "    for ax_, title_ in zip(\n",
    "        ax,\n",
    "        (\n",
    "            \"loss color\", \"rendered image\", \"rendered silhouette\",\n",
    "            \"loss silhouette\", \"target image\",  \"target silhouette\",\n",
    "        )\n",
    "    ):\n",
    "        if not title_.startswith('loss'):\n",
    "            ax_.grid(\"off\")\n",
    "            ax_.axis(\"off\")\n",
    "        ax_.set_title(title_)\n",
    "    fig.canvas.draw(); fig.show()\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd0adb4f-0373-4f86-8e3b-716800c9ba63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "90d507b0-f185-41dc-9480-d08601dd9ce2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d569727-c914-490b-ae7d-72c385680353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First move all relevant variables to the correct device.\n",
    "renderer_grid = renderer_grid.to(device)\n",
    "renderer_mc = renderer_mc.to(device)\n",
    "target_cameras = target_cameras.to(device)\n",
    "target_images = target_images.to(device)\n",
    "target_silhouettes = target_silhouettes.to(device)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Instantiate the radiance field model.\n",
    "neural_radiance_field = NeuralRadianceField(n_hidden_neurons = 350).to(device)\n",
    "\n",
    "# Instantiate the Adam optimizer. We set its master learning rate to 1e-3.\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=lr)\n",
    "\n",
    "# We sample 2 random cameras in a minibatch. Each camera\n",
    "# emits raysampler_mc.n_pts_per_image rays.\n",
    "batch_size = 2\n",
    "\n",
    "# 3000 iterations take ~20 min on a Tesla M40 and lead to\n",
    "# reasonably sharp results. However, for the best possible\n",
    "# results, we recommend setting n_iter=20000.\n",
    "n_iter = 3000\n",
    "\n",
    "# Init the loss history buffers.\n",
    "loss_history_color, loss_history_sil = [], []\n",
    "\n",
    "# The main optimization loop.\n",
    "for iteration in range(n_iter):      \n",
    "    # In case we reached the last 75% of iterations,\n",
    "    # decrease the learning rate of the optimizer 10-fold.\n",
    "    if iteration == round(n_iter * 0.75):\n",
    "        print('Decreasing LR 10-fold ...')\n",
    "        optimizer = torch.optim.Adam(\n",
    "            neural_radiance_field.parameters(), lr=lr * 0.1\n",
    "        )\n",
    "    \n",
    "    # Zero the optimizer gradient.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Sample random batch indices.\n",
    "    batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
    "    \n",
    "    # Sample the minibatch of cameras.\n",
    "    batch_cameras = FoVPerspectiveCameras(\n",
    "        R = target_cameras.R[batch_idx], \n",
    "        T = target_cameras.T[batch_idx], \n",
    "        znear = target_cameras.znear[batch_idx],\n",
    "        zfar = target_cameras.zfar[batch_idx],\n",
    "        aspect_ratio = target_cameras.aspect_ratio[batch_idx],\n",
    "        fov = target_cameras.fov[batch_idx],\n",
    "        device = device,\n",
    "    )\n",
    "    \n",
    "    # Evaluate the nerf model.\n",
    "    rendered_images_silhouettes, sampled_rays = renderer_mc(\n",
    "        cameras=batch_cameras, \n",
    "        volumetric_function=neural_radiance_field\n",
    "    )\n",
    "    rendered_images, rendered_silhouettes = (\n",
    "        rendered_images_silhouettes.split([3, 1], dim=-1)\n",
    "    )\n",
    "    \n",
    "    # Compute the silhouette error as the mean huber\n",
    "    # loss between the predicted masks and the\n",
    "    # sampled target silhouettes.\n",
    "    # silhouettes_at_rays = sample_images_at_mc_locs(\n",
    "    #     target_silhouettes[batch_idx, ..., None], \n",
    "    #     sampled_rays.xys\n",
    "    # )\n",
    "    # sil_err = huber(\n",
    "    #     rendered_silhouettes, \n",
    "    #     silhouettes_at_rays,\n",
    "    # ).abs().mean()\n",
    "    sil_err = 0\n",
    "    \n",
    "    # Compute the color error as the mean huber\n",
    "    # loss between the rendered colors and the\n",
    "    # sampled target images.\n",
    "    colors_at_rays = sample_images_at_mc_locs(\n",
    "        target_images[batch_idx], \n",
    "        sampled_rays.xys\n",
    "    )\n",
    "    color_err = huber(\n",
    "        rendered_images, \n",
    "        colors_at_rays,\n",
    "    ).abs().mean()\n",
    "    \n",
    "    # The optimization loss is a simple\n",
    "    # sum of the color and silhouette errors.\n",
    "    loss = color_err + sil_err\n",
    "    \n",
    "    # Log the loss history.\n",
    "    loss_history_color.append(float(color_err))\n",
    "    loss_history_sil.append(float(sil_err))\n",
    "    \n",
    "    # Every 10 iterations, print the current values of the losses.\n",
    "    if iteration % 10 == 0:\n",
    "        print(\n",
    "            f'Iteration {iteration:05d}:'\n",
    "            + f' loss color = {float(color_err):1.2e}'\n",
    "           # + f' loss silhouette = {float(sil_err):1.2e}'\n",
    "        )\n",
    "    \n",
    "    # Take the optimization step.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Visualize the full renders every 100 iterations.\n",
    "    if iteration % 100 == 0:\n",
    "        show_idx = torch.randperm(len(target_cameras))[:1]\n",
    "        show_full_render(\n",
    "            neural_radiance_field,\n",
    "            FoVPerspectiveCameras(\n",
    "                R = target_cameras.R[show_idx], \n",
    "                T = target_cameras.T[show_idx], \n",
    "                znear = target_cameras.znear[show_idx],\n",
    "                zfar = target_cameras.zfar[show_idx],\n",
    "                aspect_ratio = target_cameras.aspect_ratio[show_idx],\n",
    "                fov = target_cameras.fov[show_idx],\n",
    "                device = device,\n",
    "            ), \n",
    "            target_images[show_idx][0],\n",
    "            target_silhouettes[show_idx][0],\n",
    "            loss_history_color,\n",
    "            loss_history_sil,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a194d7-83f1-4bbb-865d-44a96001057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotating_nerf(neural_radiance_field, n_frames = 50):\n",
    "    logRs = torch.zeros(n_frames, 3, device=device)\n",
    "    logRs[:, 1] = torch.linspace(-3.14, 3.14, n_frames, device=device)\n",
    "    Rs = so3_exp_map(logRs)\n",
    "    Ts = torch.zeros(n_frames, 3, device=device)\n",
    "    Ts[:, 2] = 2.7\n",
    "    frames = []\n",
    "    print('Rendering rotating NeRF ...')\n",
    "    for R, T in zip(tqdm(Rs), Ts):\n",
    "        camera = FoVPerspectiveCameras(\n",
    "            R=R[None], \n",
    "            T=T[None], \n",
    "            znear=target_cameras.znear[0],\n",
    "            zfar=target_cameras.zfar[0],\n",
    "            aspect_ratio=target_cameras.aspect_ratio[0],\n",
    "            fov=target_cameras.fov[0],\n",
    "            device=device,\n",
    "        )\n",
    "        # Note that we again render with `NDCGridRaySampler`\n",
    "        # and the batched_forward function of neural_radiance_field.\n",
    "        frames.append(\n",
    "            renderer_grid(\n",
    "                cameras=camera, \n",
    "                volumetric_function=neural_radiance_field.batched_forward,\n",
    "            )[0][..., :3]\n",
    "        )\n",
    "    return torch.cat(frames)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    rotating_nerf_frames = generate_rotating_nerf(neural_radiance_field, n_frames=3*5)\n",
    "    \n",
    "image_grid(rotating_nerf_frames.clamp(0., 1.).cpu().numpy(), rows=3, cols=5, rgb=True, fill=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5bc65-4b38-4ba6-b06c-64ec33c0a8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced08858-2ca5-412c-a256-78911ba754d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
